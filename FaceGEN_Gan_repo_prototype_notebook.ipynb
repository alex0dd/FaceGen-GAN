{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FaceGEN_Gan_repo_prototype_notebook.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eBU4zUahpid"
      },
      "source": [
        "Instructions for Colab:\n",
        "\n",
        "1. Navigate to files.\n",
        "2. Press Mount drive.\n",
        "3. Select an account where kaggle.json file was placed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CoUoob4dfEk"
      },
      "source": [
        "\"\"\"\n",
        "# Run the following cells only if using Colab\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    # Setup Kaggle (to download dataset)\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp \"/content/drive/My Drive/kaggle/kaggle.json\" ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "    # Setup project repository\n",
        "    !git clone https://github.com/alexpod1000/FaceGen-GAN.git\n",
        "    %cd FaceGen-GAN/\n",
        "    !pwd\n",
        "    # Download and unpack dataset\n",
        "    !kaggle datasets download -d jessicali9530/celeba-dataset\n",
        "    !unzip celeba-dataset.zip -d data\n",
        "    # move all the images one folder upside (credits to: https://stackoverflow.com/a/11942775)\n",
        "    !find data/img_align_celeba/img_align_celeba/ -name '*.*' -exec mv --target-directory=data/img_align_celeba '{}' +\n",
        "    !rm -rf data/img_align_celeba/img_align_celeba/\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeWug8_qxLUl"
      },
      "source": [
        "%cd FaceGen-GAN/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g05SmQGjkcHA"
      },
      "source": [
        "# Actual code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJB5k_dEkP4y"
      },
      "source": [
        "import os\n",
        "\n",
        "# DATASET PATHS\n",
        "dataset_base_directory = \"./data\"\n",
        "dataset_attr_file = os.path.join(dataset_base_directory, \"list_attr_celeba.csv\")\n",
        "dataset_images_path = os.path.join(dataset_base_directory, \"img_align_celeba\")\n",
        "# SPLIT RANGES\n",
        "train_split_range = (0, 162770)\n",
        "valid_split_range = (162770, 182637)\n",
        "test_split_range = (182637, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdtnHXBv2dmW"
      },
      "source": [
        "USE_AMP = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flRm3_Z-xDkm"
      },
      "source": [
        "if USE_AMP:\n",
        "    from tensorflow.keras import mixed_precision\n",
        "    mixed_precision.set_global_policy('mixed_float16')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ba-tfutsEK9"
      },
      "source": [
        "\"\"\"\n",
        "EXPERIMENTS FILE\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "from architectures.base_dcgan import Generator, Discriminator, GAN_Wrapper\n",
        "\n",
        "# TRAINING PARAMETERS\n",
        "model_name = \"base_dcgan\"\n",
        "batch_size = 32\n",
        "n_epochs = 100\n",
        "# GENERATOR PARAMETERS\n",
        "conditional_dim = 40\n",
        "latent_dim = 128\n",
        "filters_gen = 64\n",
        "kernel_size_gen = 4\n",
        "# DISCRIMINATOR PARAMETERS\n",
        "filters_disc = 64\n",
        "kernel_size_disc = 5\n",
        "# TRAINING OPTIMIZER PARAMETERS\n",
        "init_learning_rate_gen = 0.0002\n",
        "init_learning_rate_disc = 0.0002\n",
        "beta_1 = 0.5\n",
        "# RESUME PARAMS\n",
        "load_model = False\n",
        "load_epoch = 0\n",
        "\n",
        "# create models\n",
        "generator_model = Generator(latent_dim, filters=filters_gen, kernel_size=kernel_size_gen)\n",
        "discriminator_model = Discriminator(filters=filters_disc, kernel_size=kernel_size_disc)\n",
        "# create gan wrapper model\n",
        "gan_model = GAN_Wrapper(discriminator_model, generator_model)#, use_amp=USE_AMP)\n",
        "\n",
        "# optimizers\n",
        "d_optimizer = tf.keras.optimizers.Adam(learning_rate=init_learning_rate_disc, beta_1=beta_1)\n",
        "g_optimizer = tf.keras.optimizers.Adam(learning_rate=init_learning_rate_gen, beta_1=beta_1)\n",
        "if USE_AMP:\n",
        "    d_optimizer = mixed_precision.LossScaleOptimizer(d_optimizer)\n",
        "    g_optimizer = mixed_precision.LossScaleOptimizer(g_optimizer)\n",
        "\n",
        "\n",
        "# compile model\n",
        "gan_model.compile(\n",
        "    d_optimizer=d_optimizer,\n",
        "    g_optimizer=g_optimizer,\n",
        "    loss_fn=tf.keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "# callbacks\n",
        "train_callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath='saved_models/'+model_name+'/model_{epoch}.h5', \n",
        "        save_weights_only=True\n",
        "    )\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF0_rUVUsKok"
      },
      "source": [
        "\"\"\"\n",
        "MAIN FILE\n",
        "\"\"\"\n",
        "from utils.file_utils import makedir_if_not_exists\n",
        "from utils.visualization_utils import save_plot_batch\n",
        "\n",
        "from callbacks import ImagesLoggingCallback\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# FILE PARAMETERS\n",
        "model_save_dir = \"saved_models/{}/\".format(model_name)\n",
        "model_images_save_base_dir = \"gen/{}\".format(model_name)\n",
        "model_gen_sample_dir = \"gen/{}/sample/\".format(model_name)\n",
        "model_gen_real_dir = \"gen/{}/real_cond/\".format(model_name)\n",
        "\n",
        "# make model directories if they no exist\n",
        "makedir_if_not_exists(model_save_dir)\n",
        "makedir_if_not_exists(model_gen_sample_dir)\n",
        "makedir_if_not_exists(model_gen_real_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wi7lbH3xi_A"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "from utils.image_utils import load_image\n",
        "\n",
        "class DataSequence(Sequence):\n",
        "    \"\"\"\n",
        "    Keras Sequence object to train a model on larger-than-memory data.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, data_root, batch_size, resize_size=(64, 64), flip_augment=True, mode='train', use_amp=False):\n",
        "        self.df = df\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.resize_size = resize_size\n",
        "        self.crop_pt_1 = (45, 25)\n",
        "        self.crop_pt_2 = (173, 153)\n",
        "        self.flip_augment = flip_augment\n",
        "        # extract columns from df columns\n",
        "        self.label_columns = self.df.columns[1:].tolist() \n",
        "        self.use_amp = use_amp\n",
        "\n",
        "        # Take labels and a list of image locations in memory\n",
        "        self.labels = self.df[self.label_columns].values\n",
        "        self.im_list = self.df['Image_Name'].apply(lambda x: os.path.join(data_root, x)).tolist()\n",
        "        self.img_cache = {}\n",
        "        # Trigger a shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(math.floor(len(self.df) / float(self.batch_size)))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Shuffles indexes after each epoch if in training mode\n",
        "        self.indexes = range(len(self.im_list))\n",
        "        if self.mode == 'train':\n",
        "            self.indexes = random.sample(self.indexes, k=len(self.indexes))\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return self.labels[idx]\n",
        "\n",
        "    def get_batch_features(self, idx):\n",
        "        images = []\n",
        "        for im_idx in idx:\n",
        "            im = self.im_list[im_idx]\n",
        "            if im not in self.img_cache:\n",
        "                loaded_image = load_image(im, self.resize_size, self.crop_pt_1, self.crop_pt_2)\n",
        "            else:\n",
        "                loaded_image = np.copy(self.img_cache[im])\n",
        "            if self.flip_augment and random.random() < 0.5:\n",
        "                loaded_image = np.flip(loaded_image, 1)\n",
        "            images.append(loaded_image)\n",
        "        # Fetch a batch of inputs\n",
        "        #if self.use_amp:\n",
        "        #    return np.array(images, dtype=np.float16)\n",
        "        #else:\n",
        "        return np.array(images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        idx = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # get the actual data\n",
        "        batch_x = self.get_batch_features(idx)\n",
        "        #if self.use_amp:\n",
        "        #    batch_y = np.clip(self.get_batch_labels(idx).astype(np.float16), 0, 1)\n",
        "        #else:\n",
        "        batch_y = np.clip(self.get_batch_labels(idx).astype(np.float32), 0, 1)\n",
        "        return (batch_x, batch_y), batch_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCVzahhRseVv"
      },
      "source": [
        "#from datasets.celeba.dataloader import DataSequence\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data_df = pd.read_csv(dataset_attr_file).rename(\n",
        "    columns={\"image_id\": \"Image_Name\"}\n",
        ")\n",
        "train_df = data_df[train_split_range[0]:train_split_range[1]]\n",
        "valid_df = data_df[valid_split_range[0]:valid_split_range[1]]\n",
        "test_df = data_df[test_split_range[0]:test_split_range[1]]\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" \n",
        "DATASEQ\n",
        "\"\"\"\n",
        "\n",
        "batch_size = 64 * 2\n",
        "\n",
        "training_generator = DataSequence(train_df, dataset_images_path,  batch_size=batch_size, use_amp=USE_AMP)\n",
        "\n",
        "# take first batch of validation dataset for visual results report \n",
        "# (i.e. conditioned generation based on first batch conditions)\n",
        "valid_cond_batch = DataSequence(valid_df, dataset_images_path,  batch_size=batch_size, mode=\"valid\", use_amp=USE_AMP)\n",
        "\n",
        "_, real_view_conditions = next(iter(valid_cond_batch))\n",
        "real_view_conditions = real_view_conditions[:25]\n",
        "\n",
        "# take apart a batch for reconstruction\n",
        "view_cond = np.zeros((25, conditional_dim), dtype=np.float32)\n",
        "view_cond[:, 31] = 1.0 # all smile\n",
        "view_cond = view_cond.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1k0CxNS0yS5"
      },
      "source": [
        "bbb = next(iter(training_generator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TJg6b9X01wo"
      },
      "source": [
        "bbb[0][0].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-33AJXVMwuoy"
      },
      "source": [
        "class ImagesLoggingCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, n_images, latent_dim, view_cond, real_view_conditions, images_dir):\n",
        "        super(ImagesLoggingCallback, self).__init__()\n",
        "        self.n_images = n_images\n",
        "        self.latent_dim = latent_dim\n",
        "        self.images_dir = images_dir\n",
        "        self.view_cond = view_cond\n",
        "        self.real_view_conditions = real_view_conditions\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.n_images, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors, self.view_cond, training=False)\n",
        "        generated_images_real_cond = self.model.generator(random_latent_vectors, self.real_view_conditions, training=False)\n",
        "        # convert to float32 if needed\n",
        "        generated_images = tf.cast(generated_images, tf.float32)\n",
        "        generated_images_real_cond = tf.cast(generated_images_real_cond, tf.float32)\n",
        "        # rescale\n",
        "        generated_images = (generated_images + 1) / 2.0\n",
        "        generated_images_real_cond = (generated_images_real_cond + 1) / 2.0\n",
        "        #generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        generated_images_real_cond.numpy()\n",
        "\n",
        "        save_plot_batch(generated_images, self.images_dir+\"/sample/sample_{}.png\".format(epoch))\n",
        "        save_plot_batch(generated_images_real_cond, self.images_dir+\"/real_cond/sample_{}.png\".format(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMIhu9ILr4az"
      },
      "source": [
        "train_callbacks.append(ImagesLoggingCallback(25, latent_dim, view_cond, real_view_conditions, model_images_save_base_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJeNuaVUsgNj"
      },
      "source": [
        "history = gan_model.fit(training_generator,\n",
        "    use_multiprocessing=True,\n",
        "    workers=8,\n",
        "    epochs=n_epochs,\n",
        "    callbacks=train_callbacks,\n",
        "    initial_epoch=load_epoch\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPFERINj1GDp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}